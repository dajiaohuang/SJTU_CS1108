{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据科学引论 - Python之道 </h1>\n",
    "\n",
    "<h1 align=center> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第5课 数据收集 - Python网络爬虫实践 I </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫概述\n",
    "在阅读这个样例之前，建议先了解爬虫是什么，简单理解url、爬虫技术、网页html等基本概念。\n",
    "\n",
    "在完成本笔记本的操作之前，大家可以通过命令 ```pip install scrapy``` 或 ```pip3 install scrapy``` 安装所需依赖包。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义爬虫的任务\n",
    "\n",
    "## 涉及的语法\n",
    "语法涉及类（面向对象）、列表list、字典dict、循环、函数、字符串操作、文件读写\n",
    "\n",
    "## 概述\n",
    "这个爬虫的任务是爬取http://quotes.toscrape.com/page/1/ 的前2页，提取每条名言的文字内容，作者和标签，最后以JSON格式保存到文件中\n",
    "\n",
    "\n",
    "## 如何修改\n",
    "\n",
    "在自己做定制时，只需要修改`__init__`和`parse`两个方法，通俗讲__init__方法决定了爬取哪些网站，parse则指明了在每一个网页上爬取哪些内容\n",
    "- init_urls: 设置待爬取网站的列表和保存文件路径，其中变量self.urls是待爬取网站的列表，self.file是一个文件对象\n",
    "- parse：方法内是针对每个url成功访问之后进行的页面解析\n",
    "   关于如何解析具体网页，也就是选择器的使用，与网页格式十分相关，这个样例无法适用于其他网站。由于选择器的使用有很大的选择性，所以可以参考文档http://scrapy-chs.readthedocs.io/zh_CN/latest/topics/selectors.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "class MySpider(scrapy.Spider):\n",
    "    \n",
    "    name = \"spider\"\n",
    "    allowed_domains = ['quotes.toscrape.com']\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.file = open('demo1_quotes.json', 'w');\n",
    "        \n",
    "        #设置待爬取网站列表\n",
    "        self.urls = []\n",
    "        for i in range(1,3):\n",
    "            self.urls.append('http://quotes.toscrape.com/page/' + str(i) + '/')\n",
    "            \n",
    "#       初始化效果 效果等同\n",
    "#         self.urls = [\n",
    "#             'http://quotes.toscrape.com/page/1/',\n",
    "#             'http://quotes.toscrape.com/page/2/',\n",
    "#         ]\n",
    "\n",
    "        \n",
    "    def start_requests(self):\n",
    "        for url in self.urls:\n",
    "            time.sleep(5)\n",
    "            yield scrapy.http.Request(url=url, callback=self.parse, dont_filter=True)\n",
    "    \n",
    "\n",
    "    #parse方法会在每个request收到response之后调用\n",
    "    def parse(self, response):\n",
    "        \n",
    "        #提取名言列表\n",
    "        quotes = response.css(\"div.quote\");\n",
    "        for quote in quotes:\n",
    "            #提取每条名言中的作者名\n",
    "            author = quote.css(\"small.author::text\").extract_first();\n",
    "            #提取名言的文字内容\n",
    "            text = quote.css(\".text::text\").extract_first();\n",
    "            #提取名言标签\n",
    "            tags = quote.css(\".tags a::attr(href)\").extract();\n",
    "            #构建字典对象\n",
    "            item = {\"author\":author, \"text\": text, \"tags\":tags };\n",
    "            #将字典转换成json字符串\n",
    "            line = json.dumps(dict(item))\n",
    "            #将每个条目写入文件\n",
    "            self.file.write(line + \"\\n\")\n",
    "        #及时将内容写入文件，否则可能会出现少许延迟\n",
    "        self.file.flush()\n",
    "        os.fsync(self.file)\n",
    "        #输出当前解析完成的网页网址，可以当做爬取进度来看待,与程序逻辑无关\n",
    "        print(\"over: \" + response.url)\n",
    "        \n",
    "    def close(spider, reason):\n",
    "        self.file.close();\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 执行爬虫任务\n",
    "启动后，将执行Myspider。\n",
    "这部分的代码块，如果确实非常了解scrapy的运行机制，那么可以做定制，否则不建议自行修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "from multiprocessing import Process\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "\n",
    "def run_spider():\n",
    "    process = CrawlerProcess(get_project_settings())\n",
    "    process.crawl(MySpider)\n",
    "    process.start() # 这句代码就是开始了整个爬虫过程 \n",
    "    \n",
    "run_spider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印一下看看效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []  # 存储解析后的JSON对象的列表\n",
    "\n",
    "with open(\"demo1_quotes.json\", \"r\") as json_file:\n",
    "    for line in json_file:\n",
    "        try:\n",
    "            json_data = json.loads(line)\n",
    "            data_list.append(json_data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON解析错误：{e}\")\n",
    "\n",
    "# 现在data_list中包含了所有JSON对象的列表\n",
    "for item in data_list:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.8.18 ('gym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "805927312004146b6a7d5bee169c5fc493d55b9cce04469bb19a045e8f3d6def"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
